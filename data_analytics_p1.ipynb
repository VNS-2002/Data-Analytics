{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MAin\n"
      ],
      "metadata": {
        "id": "ehSJrxIGg5uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n"
      ],
      "metadata": {
        "id": "6TkM_o9inFAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ac9575-d141-4810-e389-d6a0a44ef333"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "import nltk\n",
        "import textstat\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Read input file\n",
        "input_data = pd.read_excel('/content/Input.xlsx')\n",
        "\n",
        "# Function to extract article text from URL\n",
        "def extract_article_text(url, url_id):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract article title and text while ignoring unwanted content\n",
        "        title = soup.find('title').get_text() if soup.find('title') else ''\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = '\\n'.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        # Save extracted text into a text file with URL_ID as filename\n",
        "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(f\"Title: {title}\\n\\n{article_text}\")\n",
        "\n",
        "        return True, \"Extraction successful\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Extraction failed: {str(e)}\"\n",
        "\n",
        "# Loop through URLs in the input file and extract article text\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    success, message = extract_article_text(url, url_id)\n",
        "    print(f\"URL ID: {url_id} - {message}\")\n",
        "\n",
        "# Function for text analysis and computing variables\n",
        "def analyze_text(text):\n",
        "    # Load positive and negative words list\n",
        "    positive_words = pd.read_csv('/content/positive-words.csv', header=None)[0].tolist()\n",
        "    negative_words = pd.read_csv('/content/negative-words.csv', header=None)[0].tolist()\n",
        "\n",
        "    # Load additional stop words lists\n",
        "    # Load additional stop words lists\n",
        "    stop_words_files = [\n",
        "        '/content/StopWords_Auditor.csv', '/content/StopWords_Currencies.csv', '/content/StopWords_DatesandNumbers.csv',\n",
        "        '/content/StopWords_Generic.csv', '/content/StopWords_GenericLong.csv', '/content/StopWords_Geographic.csv', '/content/StopWords_Names.csv'\n",
        "    ]\n",
        "    additional_stopwords = []\n",
        "    for file in stop_words_files:\n",
        "        data = pd.read_csv(file, header=None)  # Read each CSV file\n",
        "        additional_stopwords += data[0].tolist()  # Concatenate the content of each file to additional_stopwords\n",
        "\n",
        "\n",
        "    # Tokenize text\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english') + additional_stopwords)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Calculate variables\n",
        "    positive_score = sum(word in positive_words for word in words)\n",
        "    negative_score = sum(word in negative_words for word in words)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = TextBlob(text).sentiment.subjectivity\n",
        "    sentences = TextBlob(text).sentences\n",
        "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
        "    words_count = len(words)\n",
        "    # complex_words = [word for word in words if len(Word(word).syllables) > 2]\n",
        "    complex_words = [word for word in words if textstat.syllable_count(word) > 2]\n",
        "    percentage_complex_words = (len(complex_words) / len(words)) * 100\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_words_per_sentence = words_count / len(sentences)\n",
        "    # syllables_per_word = sum(len(Word(word).syllables) for word in words) / len(words)\n",
        "    syllables_per_word = sum(textstat.syllable_count(word) for word in words) / len(words)\n",
        "\n",
        "    personal_pronouns = sum(1 for word in words if word.lower() in ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'])\n",
        "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
        "\n",
        "    return [\n",
        "        positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
        "        percentage_complex_words, fog_index, avg_words_per_sentence, len(complex_words),\n",
        "        words_count, syllables_per_word, personal_pronouns, avg_word_length\n",
        "    ]\n",
        "\n",
        "# Analyze text from extracted files and store computed variables\n",
        "output_data = []\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    try:\n",
        "        with open(f\"{url_id}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read()\n",
        "            computed_variables = analyze_text(text)\n",
        "            output_row = [url_id] + computed_variables\n",
        "            output_data.append(output_row)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"File {url_id}.txt not found\")\n",
        "\n",
        "# Create DataFrame for output data and save it to Excel\n",
        "columns = [\n",
        "    \"URL_ID\", \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "    \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\", \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
        "    \"COMPLEX WORD COUNT\", \"WORD COUNT\", \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
        "]\n",
        "\n",
        "output_df = pd.DataFrame(output_data, columns=columns)\n",
        "output_df.to_excel('Output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "TYPlWhQ4g5Vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39edf17-fc55-4c7a-f67d-e7afea6548b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL ID: test01 - Extraction successful\n",
            "URL ID: test02 - Extraction successful\n",
            "URL ID: test03 - Extraction successful\n",
            "URL ID: test04 - Extraction successful\n",
            "URL ID: test05 - Extraction successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S63ZU__cmBI4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}